{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Perceiver Solo Piano Maker (Version 2022.12.17)\n",
    "\n",
    "https://github.com/asigalov61/tegridy-tools\n",
    "\n",
    "https://github.com/lucidrains/perceiver-ar-pytorch\n",
    "\n",
    "## Project Los Angeles\n",
    "## Tegridy Code 2022\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "L82QbwLTPclr",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/astais/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\n",
      "Tue Nov 22 13:07:36 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:02:00.0 Off |                  N/A |\n",
      "| 25%   31C    P8     7W / 180W |      8MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:03:00.0 Off |                  N/A |\n",
      "| 36%   64C    P2    52W / 180W |   6328MiB /  8192MiB |     15%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      2370      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A      2370      G   /usr/lib/xorg/Xorg                  9MiB |\n",
      "|    1   N/A  N/A   3302976      C   ...nda3/envs/nocd/bin/python      935MiB |\n",
      "|    1   N/A  N/A   4037465      C   ...a3/envs/mGENRE/bin/python     5379MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "# !export CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install dependencies and import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: /home/astais/miniconda3/envs/tf/lib/libtinfo.so.6: no version information available (required by /bin/bash)\r\n",
      "/bin/bash: line 0: cd: /home/astais/Perceiver-Music-Transformer/Training-Code: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "!cd '/home/astais/Perceiver-Music-Transformer/Training-Code'\n",
    "# !git clone https://github.com/asigalov61/tegridy-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cQwiG4rrUXba"
   },
   "outputs": [],
   "source": [
    "# !pip install torch torch-summary einops sklearn matplotlib tqdm\n",
    "# If it fails due to no space left on device use prefix\n",
    "# TMPDIR=/var/tmp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the above don't work\n",
    "# !pip install torch==1.12.1+cu102 torchvision==0.13.1+cu102 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu102\n",
    "# !pip3 install -U scikit-learn scipy matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0L6S9kv7hlTr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading modules...\n",
      "/home/astais/perceiver-ar/Training-Code/tegridy-tools/tegridy-tools\n",
      "/home/astais/perceiver-ar/Training-Code/tegridy-tools/tegridy-tools/Perceiver-AR\n",
      "/home/astais/perceiver-ar/Training-Code\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Load modules and make data dir\n",
    "\n",
    "print('Loading modules...')\n",
    "\n",
    "import pickle,os,random,secrets,datetime\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchsummary import summary\n",
    "from sklearn import metrics\n",
    "\n",
    "%cd /home/astais/perceiver-ar/Training-Code/tegridy-tools/tegridy-tools/\n",
    "\n",
    "import TMIDIX\n",
    "\n",
    "%cd /home/astais/perceiver-ar/Training-Code/tegridy-tools/tegridy-tools/Perceiver-AR/\n",
    "\n",
    "from perceiver_ar_pytorch import PerceiverAR\n",
    "from autoregressive_wrapper import AutoregressiveWrapper\n",
    "\n",
    "%cd /home/astais/perceiver-ar/Training-Code/\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Training Dataset and Directory Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Change according to training dataset and basic load,save dir\n",
    "dataset_name='maestro-v3.0.0'\n",
    "basic_dir=\"/data/share/share1/users/astais/\"\n",
    "\n",
    "current_time = datetime.datetime.now().strftime('%Y%m%d')\n",
    "preprocess_addr = basic_dir+\"Unprocessed-Datasets/\"+dataset_name\n",
    "dataset_addr=basic_dir+\"Processed-Datasets/perceiver-ar/perceiver-ar_\"+dataset_name\n",
    "base_save_dir=basic_dir+'Saved-Models/perceiver-ar/'\n",
    "train_out_dir=basic_dir+'Training-Ouputs/perceiver-ar/'+'perceiver-ar_'+dataset_name+'_'+current_time+'/'\n",
    "\n",
    "for addr in [dataset_addr,train_out_dir]:\n",
    "    if not os.path.exists(addr):\n",
    "        os.makedirs(addr)\n",
    "        \n",
    "print(\"Done!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Preprocess Training Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Process MIDIs with TMIDIX MIDI Processor\n",
    "\n",
    "sorted_or_random_file_loading_order = False # Sorted order is NOT usually recommended\n",
    "dataset_ratio = 1 # Change this if you need more data\n",
    "\n",
    "print('TMIDIX MIDI Processor')\n",
    "print('Starting up...')\n",
    "###########\n",
    "files_count = 0\n",
    "gfiles = []\n",
    "train_data1 = []\n",
    "###########\n",
    "\n",
    "print('Loading MIDI files...')\n",
    "print('This may take a while on a large dataset in particular.')\n",
    "\n",
    "# os.chdir(dataset_addr)\n",
    "filez = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(preprocess_addr):\n",
    "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
    "print('=' * 70)\n",
    "\n",
    "if filez == []:\n",
    "    print('Could not find any MIDI files. Please check Dataset dir...')\n",
    "    print('=' * 70)\n",
    "if sorted_or_random_file_loading_order:\n",
    "    print('Sorting files...')\n",
    "    filez.sort()\n",
    "    print('Done!')\n",
    "    print('=' * 70)\n",
    "else:\n",
    "    print('Randomizing file list...')\n",
    "    random.shuffle(filez)\n",
    "\n",
    "print('Processing MIDI files. Please wait...')\n",
    "for f in tqdm(filez[:int(len(filez) * dataset_ratio)]):\n",
    "    try:\n",
    "        fn = os.path.basename(f)\n",
    "        fn1 = fn.split('.')[0]\n",
    "        #print('Loading MIDI file...')\n",
    "        score = TMIDIX.midi2ms_score(open(f, 'rb').read())\n",
    "        events_matrix = []\n",
    "        itrack = 1\n",
    "        while itrack < len(score):\n",
    "            for event in score[itrack]:         \n",
    "                if event[0] == 'note' and event[3] != 9:\n",
    "                    events_matrix.append(event)\n",
    "            itrack += 1\n",
    "        \n",
    "        if len(events_matrix) > 0:\n",
    "\n",
    "          # Sorting...\n",
    "          events_matrix.sort(key=lambda x: x[4], reverse=True)\n",
    "          events_matrix.sort(key=lambda x: x[1])\n",
    "          # recalculating timings\n",
    "          for e in events_matrix:\n",
    "              e[1] = int(e[1] / 10)\n",
    "              e[2] = int(e[2] / 20) \n",
    "          # final processing...\n",
    "          train_data1.extend([126+0, 126+128, 0+256, 0+384]) # Intro/Zero seq\n",
    "          pe = events_matrix[0]\n",
    "     \n",
    "          for e in events_matrix:\n",
    "              time = max(0, min(126, e[1]-pe[1]))\n",
    "              dur = max(1, min(126, e[2]))\n",
    "              ptc = max(1, min(126, e[4]))\n",
    "              vel = max(1, min(126, e[5]))\n",
    "              train_data1.extend([time+0, dur+128, ptc+256, vel+384])\n",
    "              pe = e\n",
    "\n",
    "          files_count += 1\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print('Quitting...')\n",
    "        break  \n",
    "    except:\n",
    "        print('Bad MIDI:', f)\n",
    "        continue\n",
    "\n",
    "print('=' * 70)\n",
    "TMIDIX.Tegridy_Any_Pickle_File_Writer(train_data1, dataset_addr+dataset_name+'/perceiver-ar_'+dataset_name)        \n",
    "\n",
    "print('Done!')   \n",
    "print('=' * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Download and unzip training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perceiver Solo Piano Training Data Pack\n",
    "# %cd /data/ironman/data1/users/astais/Processed-Datasets/\n",
    "# # !wget --no-check-certificate -O 'Perceiver-Piano-Training-Data.zip' \"https://onedrive.live.com/download?cid=8A0D502FC99C608F&resid=8A0D502FC99C608F%2118740&authkey=ANEK-9WanNFyalw\"\n",
    "# !unzip 'Perceiver-Piano-Training-Data.zip'\n",
    "# # !rm 'Perceiver-Piano-Training-Data.zip'\n",
    "# %cd /home/astais/Perceiver-Music-Transformer/Training-Code/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "filez = list()\n",
    "for (dirpath, dirnames, filenames) in os.walk(dataset_addr):\n",
    "    filez += [os.path.join(dirpath, file) for file in filenames]\n",
    "print('=' * 70)\n",
    "filez.sort()\n",
    "print('Loading training data... Please wait...')\n",
    "train_data = torch.Tensor()\n",
    "\n",
    "for f in filez:\n",
    "    train_data = torch.cat((train_data, torch.Tensor(pickle.load(open(f, 'rb')))))\n",
    "    print('Loaded file:', f)\n",
    "    \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)\n",
    "print(len(train_data)//(4096 * 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[:15], train_data[-15:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# Setup model\n",
    "\n",
    "# constants\n",
    "SEQ_LEN = 4096 * 4 # Total of 16k\n",
    "PREFIX_SEQ_LEN = (4096 * 4) - 1024 # 15.3k\n",
    "BATCH_SIZE = 1  # initial 4\n",
    "NUM_BATCHES = 20*len(train_data) // SEQ_LEN // BATCH_SIZE # change multiply ratio based on dataset dimension\n",
    "GRADIENT_ACCUMULATE_EVERY = 4\n",
    "LEARNING_RATE = 2e-5 # initial 2e-4\n",
    "VALIDATE_EVERY  = 100 \n",
    "GENERATE_EVERY  = 200 \n",
    "GENERATE_LENGTH = 32 \n",
    "\n",
    "# helpers\n",
    "def cycle(loader):\n",
    "    while True:\n",
    "        for data in loader:\n",
    "            yield data\n",
    "\n",
    "# instantiate model\n",
    "model = PerceiverAR(\n",
    "    num_tokens = 512, # feizc 256\n",
    "    dim = 1024, # feizc 512\n",
    "    depth = 24, # initial 24, feizc 8\n",
    "    heads = 16, # original 16, feizc 8\n",
    "    dim_head = 64,\n",
    "    cross_attn_dropout = 0.5,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
    ")\n",
    "\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "print('Done!')\n",
    "      \n",
    "summary(model)\n",
    "\n",
    "# prepare enwik8 data\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __getitem__(self, index): \n",
    "        # random sampling\n",
    "        # idx = secrets.randbelow((self.data.size(0) // (self.seq_len))-1) * (self.seq_len)\n",
    "        \n",
    "        # consequtive sampling seems to be better at 64k seq_len\n",
    "        idx = index * self.seq_len\n",
    "        full_seq = self.data[idx: idx + self.seq_len + 1].long()\n",
    "        return full_seq.cuda()\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.data.size(0) // self.seq_len)-1\n",
    "\n",
    "train_dataset = MusicDataset(train_data, SEQ_LEN)\n",
    "val_dataset   = MusicDataset(train_data, SEQ_LEN)\n",
    "train_loader  = cycle(DataLoader(train_dataset, batch_size = BATCH_SIZE))\n",
    "val_loader    = cycle(DataLoader(val_dataset, batch_size = BATCH_SIZE))\n",
    "\n",
    "# optimizer\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "-BoaqJAOU5M8"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QauuLTxFTDXw",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Variables for saving best evalaution loss,acc\n",
    "best_val_acc        = 0.0\n",
    "best_val_acc_step  = -1\n",
    "best_val_loss       = float(\"inf\")\n",
    "best_val_loss_step = -1\n",
    "# best_acc_file = '/data/data1/users/astais/Saved-Models/perceiver-ar/'+dataset_name+'_best-val-acc.pth'\n",
    "# best_loss_file = '/data/data1/users/astais/Saved-Models/perceiver-ar/'+dataset_name+'_best-val-loss.pth'\n",
    "\n",
    "# Training Loop\n",
    "for i in tqdm(range(NUM_BATCHES), mininterval=10., desc='Training'):\n",
    "    new_best = False\n",
    "    model.train()\n",
    "\n",
    "    for __ in range(GRADIENT_ACCUMULATE_EVERY):\n",
    "        loss, acc = model(next(train_loader))\n",
    "        loss.backward()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    train_accs.append(acc.item())\n",
    "    \n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "       \n",
    "    if i % VALIDATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss, val_acc = model(next(val_loader))\n",
    "#             print(f'Validation loss: {val_loss.item()}')\n",
    "#             print(f'Validation acc: {val_acc.item()}')\n",
    "            val_losses.append(val_loss.item())\n",
    "            val_accs.append(val_acc.item())\n",
    "\n",
    "  \n",
    "    if i % GENERATE_EVERY == 0:\n",
    "        model.eval()\n",
    "        inp = random.choice(val_dataset)[:-1]\n",
    "        sample = model.generate(inp[None, ...], GENERATE_LENGTH)\n",
    "    \n",
    "    # Save Model & Plot Graphs: Trained for necessery batches and new evaluation loss or accuracy best\n",
    "    if i % VALIDATE_EVERY == 0 and i>NUM_BATCHES//40  and (val_acc > best_val_acc or val_loss < best_val_loss) and (val_acc>0.8 or val_loss<0.2):\n",
    "\n",
    "        # Check if val_acc or val_loss are best so far\n",
    "        if(val_acc > best_val_acc and val_acc>0.8):\n",
    "                best_val_acc = val_acc\n",
    "                best_val_acc_step  = i\n",
    "                best_acc_file = base_save_dir+'perceiver-ar_'+dataset_name+'_'+ str(i) + '_steps_' + str(round(float(val_accs[-1]), 4)) + '_acc.pth'\n",
    "                torch.save(model.state_dict(), best_acc_file)\n",
    "\n",
    "        if(val_loss < best_val_loss and val_loss<0.2):\n",
    "                best_val_loss       = val_loss\n",
    "                best_val_loss_step = i\n",
    "                best_loss_file = base_save_dir+'perceiver-ar_'+dataset_name+'_'+ str(i) + '_steps_' + str(round(float(val_losses[-1]), 4)) + '_loss.pth'\n",
    "                torch.save(model.state_dict(), best_loss_file)\n",
    "        print('Saved model progress.')\n",
    "\n",
    "        print(\"Best val acc step:\", best_val_acc_step)\n",
    "        print(\"Best val acc:\", best_val_acc)\n",
    "        print(\"Best val loss step:\", best_val_loss_step)\n",
    "        print(\"Best val loss:\", best_val_loss)\n",
    "        print('Done!')\n",
    "        \n",
    "        # Plotting Training, Evaluation Graphs\n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "        ax.plot([i for i in range(len(train_losses))] ,train_losses, '#607B8B')\n",
    "        ax.set(xlabel='Steps', ylabel ='Training Loss', title='Perceiver-AR '+dataset_name+': Training Loss')\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.grid(which='minor', alpha=0.2)\n",
    "        ax.grid(which='major', alpha=0.5)\n",
    "        fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_train-loss_'+str(i) + '_steps.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Training acc graph\n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "        ax.plot([i for i in range(len(train_accs))] ,train_accs, '#607B8B')\n",
    "        ax.set(xlabel='Steps', ylabel ='Training Accuracy', title='Perceiver-AR '+dataset_name+': Training Accuracy')\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.grid(which='minor', alpha=0.2)\n",
    "        ax.grid(which='major', alpha=0.5)\n",
    "        fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_train-acc_'+str(i) + '_steps.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Validation loss graph\n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "        ax.plot([100*i for i in range(len(val_losses))] ,val_losses, '#5D478B')\n",
    "        ax.set(xlabel='Steps', ylabel ='Validation Loss', title='Perceiver-AR '+dataset_name+': Validation Loss')\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.grid(which='minor', alpha=0.2)\n",
    "        ax.grid(which='major', alpha=0.5)\n",
    "        fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_val-loss_'+str(i) + '_steps.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Validation acc graph\n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "        plt.plot([100*i for i in range(len(val_accs))] ,val_accs, '#5D478B')\n",
    "        ax.set(xlabel='Steps', ylabel ='Validation Accuracy', title='Perceiver-AR '+dataset_name+': Validation Accuracy')\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.grid(which='minor', alpha=0.2)\n",
    "        ax.grid(which='major', alpha=0.5)\n",
    "        fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_val-acc_'+str(i) + '_steps.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Training-Validation Loss Graph\n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "        x_train=np.array([i for i in range(len(train_losses))])\n",
    "        y_train=np.array(train_losses)\n",
    "        ax.plot(x_train,y_train, '#607B8B')\n",
    "\n",
    "        x_val=np.array([100*i for i in range(len(val_losses))])\n",
    "        y_val=np.array(val_losses)\n",
    "        ax.plot(x_val,y_val, '#5D478B')\n",
    "\n",
    "        ax.set(xlabel='Steps', ylabel ='Loss', title='Perceiver-AR '+dataset_name+': Training-Validation Loss')\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.legend(['Training Loss','Validation Loss'], loc='upper right')\n",
    "        ax.grid(which='minor', alpha=0.2)\n",
    "        ax.grid(which='major', alpha=0.5)\n",
    "        fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_train,val-loss_'+str(i) + '_steps.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Training-Validation Accuracy Graph\n",
    "        fig, ax = plt.subplots(dpi=300)\n",
    "        x_train=np.array([i for i in range(len(train_accs))])\n",
    "        y_train=np.array(train_accs)\n",
    "        ax.plot(x_train,y_train, '#607B8B')\n",
    "\n",
    "        x_val=np.array([100*i for i in range(len(val_accs))])\n",
    "        y_val=np.array(val_accs)\n",
    "        ax.plot(x_val,y_val, '#5D478B')\n",
    "\n",
    "        ax.set(xlabel='Steps', ylabel ='Accuracy', title='Perceiver-AR '+dataset_name+': Training-Validation Accuracy')\n",
    "        ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "        ax.legend(['Training Accuracy','Validation Accuracy'], loc='lower right')\n",
    "        ax.grid(which='minor', alpha=0.2)\n",
    "        ax.grid(which='major', alpha=0.5)\n",
    "        fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_train,val-acc_'+str(i) + '_steps.png')\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "    # Early Stopping: If best val_loss or acc hasn't appeared for 1000 batches stop training.\n",
    "    ### Add minimum step to apply early stopping\n",
    "    if(val_acc < best_val_acc and val_loss < best_val_loss and i>best_val_loss_step+1000 and i>best_val_acc_step+1000):\n",
    "        print(\"Early Stopping: Training Finished!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save stats graphs and value arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Save training, validation value arrays\n",
    "with open(train_out_dir+'perceiver-ar_'+dataset_name+\"_\"+current_time+'.npy', 'wb') as f:\n",
    "    np.save(f, np.array(train_losses))\n",
    "    np.save(f, np.array(train_accs))\n",
    "    np.save(f, np.array(val_losses))\n",
    "    np.save(f, np.array(val_accs))\n",
    "\n",
    "# Training loss graph\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "ax.plot([i for i in range(len(train_losses))] ,train_losses, '#607B8B')\n",
    "ax.set(xlabel='Steps', ylabel ='Training Loss', title='Perceiver-AR '+dataset_name+': Training Loss')\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.grid(which='minor', alpha=0.2)\n",
    "ax.grid(which='major', alpha=0.5)\n",
    "fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_train-loss.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Training acc graph\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "ax.plot([i for i in range(len(train_accs))] ,train_accs, '#607B8B')\n",
    "ax.set(xlabel='Steps', ylabel ='Training Accuracy', title='Perceiver-AR '+dataset_name+': Training Accuracy')\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.grid(which='minor', alpha=0.2)\n",
    "ax.grid(which='major', alpha=0.5)\n",
    "fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_train-acc.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Validation loss graph\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "ax.plot([100*i for i in range(len(val_losses))] ,val_losses, '#5D478B')\n",
    "ax.set(xlabel='Steps', ylabel ='Validation Loss', title='Perceiver-AR '+dataset_name+': Validation Loss')\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.grid(which='minor', alpha=0.2)\n",
    "ax.grid(which='major', alpha=0.5)\n",
    "fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_val-loss.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Validation acc graph\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "plt.plot([100*i for i in range(len(val_accs))] ,val_accs, '#5D478B')\n",
    "ax.set(xlabel='Steps', ylabel ='Validation Accuracy', title='Perceiver-AR '+dataset_name+': Validation Accuracy')\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.grid(which='minor', alpha=0.2)\n",
    "ax.grid(which='major', alpha=0.5)\n",
    "fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_val-acc.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Training-Validation Loss Graph\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "x_train=np.array([i for i in range(len(train_losses))])\n",
    "y_train=np.array(train_losses)\n",
    "ax.plot(x_train,y_train, '#607B8B')\n",
    "\n",
    "x_val=np.array([100*i for i in range(len(val_losses))])\n",
    "y_val=np.array(val_losses)\n",
    "ax.plot(x_val,y_val, '#5D478B')\n",
    "\n",
    "ax.set(xlabel='Steps', ylabel ='Loss', title='Perceiver-AR '+dataset_name+': Training-Validation Loss')\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.legend(['Training Loss','Validation Loss'], loc='upper right')\n",
    "ax.grid(which='minor', alpha=0.2)\n",
    "ax.grid(which='major', alpha=0.5)\n",
    "fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_train,val-loss.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Training-Validation Accuracy Graph\n",
    "fig, ax = plt.subplots(dpi=300)\n",
    "x_train=np.array([i for i in range(len(train_accs))])\n",
    "y_train=np.array(train_accs)\n",
    "ax.plot(x_train,y_train, '#607B8B')\n",
    "\n",
    "x_val=np.array([100*i for i in range(len(val_accs))])\n",
    "y_val=np.array(val_accs)\n",
    "ax.plot(x_val,y_val, '#5D478B')\n",
    "\n",
    "ax.set(xlabel='Steps', ylabel ='Accuracy', title='Perceiver-AR '+dataset_name+': Training-Validation Accuracy')\n",
    "ax.xaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.yaxis.set_minor_locator(AutoMinorLocator(5))\n",
    "ax.legend(['Training Accuracy','Validation Accuracy'], loc='lower right')\n",
    "ax.grid(which='minor', alpha=0.2)\n",
    "ax.grid(which='major', alpha=0.5)\n",
    "fig.savefig(train_out_dir+'perceiver-ar_'+dataset_name+'_train,val-acc.png')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load/Reload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "fname=base_save_dir+\"perceiver-ar_ailabs1k17_10300_steps_0.0069_loss.pth\"\n",
    "# constants\n",
    "SEQ_LEN = 4096 * 4 # Total of 16k\n",
    "PREFIX_SEQ_LEN = (4096 * 4) - 1024 # 15.3k\n",
    "\n",
    "model = PerceiverAR(\n",
    "    num_tokens = 512,\n",
    "    dim = 1024,\n",
    "    depth = 24, #24,\n",
    "    heads = 16, \n",
    "    dim_head = 64,\n",
    "    cross_attn_dropout = 0.5,\n",
    "    max_seq_len = SEQ_LEN,\n",
    "    cross_attn_seq_len = PREFIX_SEQ_LEN\n",
    ")\n",
    "model = AutoregressiveWrapper(model)\n",
    "model.cuda()\n",
    "\n",
    "state_dict = torch.load(fname)\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Model stats\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Token Embeddings\n",
    "\n",
    "cos_sim = metrics.pairwise.cosine_similarity(\n",
    "    model.net.token_emb.weight.cpu().detach().numpy()[0].reshape(-1, 1)\n",
    ")\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(cos_sim, cmap=\"inferno\", interpolation=\"none\")\n",
    "im_ratio = cos_sim.shape[0] / cos_sim.shape[1]\n",
    "plt.colorbar(fraction=0.046 * im_ratio, pad=0.04)\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.tight_layout()\n",
    "plt.plot()\n",
    "plt.savefig(train_out_dir+\"perceiver-ar_\"+dataset_name+\"_positional-embeddings.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MIDI option\n",
    "\n",
    "print('Loading custom MIDI file...')\n",
    "\n",
    "f = '/home/astais/Perceiver-Music-Transformer/Training-Code/tegridy-tools/tegridy-tools/seed2.mid'\n",
    "\n",
    "score = TMIDIX.midi2ms_score(open(f, 'rb').read())\n",
    "\n",
    "events_matrix = []\n",
    "\n",
    "itrack = 1\n",
    "\n",
    "while itrack < len(score):\n",
    "    for event in score[itrack]:         \n",
    "        if event[0] == 'note' and event[3] != 9:\n",
    "            events_matrix.append(event)\n",
    "    itrack += 1\n",
    "\n",
    "if len(events_matrix) > 0:\n",
    "\n",
    "    # Sorting...\n",
    "    events_matrix.sort(key=lambda x: x[4], reverse=True)\n",
    "    events_matrix.sort(key=lambda x: x[1])\n",
    "\n",
    "    # recalculating timings\n",
    "    for e in events_matrix:\n",
    "        e[1] = int(e[1] / 10)\n",
    "        e[2] = int(e[2] / 20)\n",
    "\n",
    "    # final processing...\n",
    "    inputs = []\n",
    "    \n",
    "    inputs.extend([126+0, 126+128, 0+256, 0+384]) # Intro/Zero sequence\n",
    "\n",
    "    pe = events_matrix[0]\n",
    "    for e in events_matrix:\n",
    "\n",
    "        time = max(0, min(126, e[1]-pe[1]))\n",
    "        dur = max(1, min(126, e[2]))\n",
    "\n",
    "        ptc = max(1, min(126, e[4]))\n",
    "        vel = max(1, min(126, e[5]))\n",
    "\n",
    "        inputs.extend([time+0, dur+128, ptc+256, vel+384])\n",
    "\n",
    "        pe = e\n",
    "        \n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I-Nh0Vw6hqhi"
   },
   "outputs": [],
   "source": [
    "# Generate\n",
    "\n",
    "import time\n",
    "\n",
    "model.eval()\n",
    "inp = val_dataset[0]\n",
    "\n",
    "print(inp)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "out = model.generate(inp[None, ...], \n",
    "                     128, \n",
    "                     temperature=0.6)\n",
    "\n",
    "print(time.time() - start_time, \"seconds\")\n",
    "print(out)\n",
    "\n",
    "out1 = out.cpu().tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "evZLAi-8e_Ok"
   },
   "outputs": [],
   "source": [
    "# Convert to MIDI\n",
    "if len(out1) != 0:\n",
    "    \n",
    "    song = out1\n",
    "    song_f = []\n",
    "    time = 0\n",
    "    dur = 0\n",
    "    vel = 0\n",
    "    pitch = 0\n",
    "    channel = 0\n",
    "\n",
    "    son = []\n",
    "\n",
    "    song1 = []\n",
    "\n",
    "    for s in song:\n",
    "      if s > 127:\n",
    "        son.append(s)\n",
    "\n",
    "      else:\n",
    "        if len(son) == 4:\n",
    "          song1.append(son)\n",
    "        son = []\n",
    "        son.append(s)\n",
    "    \n",
    "    for s in song1:\n",
    "\n",
    "        channel = 0 # Piano\n",
    "\n",
    "        time += s[0] * 10\n",
    "            \n",
    "        dur = (s[1]-128) * 20\n",
    "        \n",
    "        pitch = (s[2]-256)\n",
    "\n",
    "        vel = (s[3]-384)\n",
    "                                  \n",
    "        song_f.append(['note', time, dur, channel, pitch, vel ])\n",
    "\n",
    "    detailed_stats = TMIDIX.Tegridy_SONG_to_MIDI_Converter(song_f,\n",
    "                                                        output_signature = 'Perceiver',  \n",
    "                                                        output_file_name = '/data/ironman/data1/users/astais/Midi-Outputs/perceiver-ar/perceiver-ar_'+dataset_name+'_'+current_time, \n",
    "                                                        track_name='Project Los Angeles',\n",
    "                                                        list_of_MIDI_patches=[0, 24, 32, 40, 42, 46, 56, 71, 73, 0, 53, 19, 0, 0, 0, 0],\n",
    "                                                        number_of_ticks_per_quarter=500)\n",
    "\n",
    "    print('Done!')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3c380be750c5e6af527ceaac80574712fc4605a2605c2f00cf67db89e2a6bf26"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
